{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acf7d1-e7b8-4c67-9dab-53823bd78cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "# install an English Spacy model\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a7ef3-446c-4671-9727-4a30d052f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# import libraries that we'll need\n",
    "import os, re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import textacy\n",
    "from textacy import preprocessing\n",
    "from os.path import isfile, join\n",
    "from functools import partial\n",
    "\n",
    "import difflib\n",
    "\n",
    "import contextualSpellCheck\n",
    "\n",
    "# Textract can extract text from a variety of formats, including images\n",
    "# (although OCR results might be a little dodgy)\n",
    "import textract\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b2797-d972-4a40-9205-03d5123a42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "data_directory = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba11743-fa8b-43dd-b0aa-17348b7fa7d8",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning\n",
    "\n",
    "Define a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9df1c-0870-43e7-9691-ae34214a9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.make_pipeline(\n",
    "    # Normalize words in text that have been split across lines by a hyphen for visual consistency\n",
    "    # (aka hyphenated) by joining the pieces back together, sans hyphen and whitespace\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    partial(preprocessing.replace.urls, repl = \"OOV_URL\"),\n",
    "    partial(preprocessing.normalize.repeating_chars, chars = \" \"),\n",
    "    # Remove whitespace at the end of each line of the PDFs\n",
    "    lambda x: re.sub(\"\\s+\", \" \", x),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e946cc-a394-4996-840a-4d6dfaa30c68",
   "metadata": {},
   "source": [
    "# Importing and converting corpus files\n",
    "\n",
    "Reads all `.pdf` and `.docx` files from the `/data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28818a5a-d022-4357-9a66-d8f9363f0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_load = [join(data_directory, f) for f in os.listdir(data_directory) if isfile(join(data_directory, f))]\n",
    "unprocessed_docs = []\n",
    "\n",
    "for file in to_load:\n",
    "    raw_text = textract.process(file).decode()\n",
    "    unprocessed_docs.append(preproc(raw_text))\n",
    "\n",
    "processed_docs = nlp.pipe(unprocessed_docs)\n",
    "processed_docs = list(processed_docs)\n",
    "\n",
    "# Show a small preview for each document in the corpus\n",
    "[doc._.preview for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16581ea9-8363-493c-aa37-88455cbfdd8e",
   "metadata": {},
   "source": [
    "Show all `ContentexualSpellcheck` suggestions. They're made by an ML model, and are sometimes very strange. It tries to correct all words that the spacy model hasn't seen before based on their surrounding context. None of the corrections have been applied to the text in this notebook, but it is not hard to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160fc4b8-4054-4bb6-bff4-a87a4a89e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.suggestions_spellCheck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f1aa8-1281-48bc-8ffb-45acb670c37d",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646fee2-af8f-4d98-9125-3394e8bc9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first document\n",
    "doc = processed_docs[1]\n",
    "sentence_spans = list(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be525228-34ef-4014-8c83-5b874e3f0abc",
   "metadata": {},
   "source": [
    "## Named entitities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b936c615-c321-4056-9aa0-5cb31368f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only showing first 200 tokens for convienience\n",
    "displacy.render(doc[:200], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258b5bc-ccf8-4d58-b027-ab756a9ba34e",
   "metadata": {},
   "source": [
    "## Dependancy parse treesentence_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df43fab-c998-41c6-964c-39f7265c4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sentence only\n",
    "displacy.render(sentence_spans[0], style=\"dep\", options = { \"compact\": True })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b71f5-3252-4569-9d81-0615dffc8c5c",
   "metadata": {},
   "source": [
    "# Ngrams\n",
    "\n",
    "List all bi- and trigrams that occur at least twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23691db1-4072-49e6-8041-71b5f5cc5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = list(textacy.extract.basics.ngrams(doc, (2, 3), min_freq = 2))\n",
    "\n",
    "# list(ngrams)[0].text\n",
    "\n",
    "dir(ngrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7290a11-c255-4191-b469-e2c531e7485a",
   "metadata": {},
   "source": [
    "# Pattern matching\n",
    "\n",
    "Find all instances of adjective + noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308642b-cbef-4f9a-ae54-28c6bffd6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your patterns\n",
    "patterns = {\n",
    "    \"adj_noun\": [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "    \"adj_propn\": [{\"POS\": \"ADJ\"}, {\"POS\": \"PROPN\"}],\n",
    "}\n",
    "\n",
    "# add the patterns to the matcher\n",
    "for pattern_name, pattern in patterns.items():\n",
    "    matcher.add(pattern_name, [pattern])\n",
    "    \n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get string representation\n",
    "    pattern_name = nlp.vocab.strings[match_id]\n",
    "    # Get text of match\n",
    "    span = doc[start:end]\n",
    "    print(f\"{span.text} -- {pattern_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e9e9d-de47-46d6-b990-35b6191a15ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
